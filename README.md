**ğŸ§¾ ğŸ¯ Project Title: LLM Fine Tuning and Evaluation - Llama 3.2  
ğŸ“… Project Timeline: October 2024 â€“ February 2025**  
ğŸ¥ YouTube Demo: Not available  
ğŸ“¦ GitHub Source Code: <https://github.com/IvanSicaja/2024.12.27_GitHub_LLM-Fine-Tuning-and-Evaluation---Llama-3.2>

\----------------------------------------------------------------------------------------------------------------

ğŸ·ï¸ My Personal Profiles: â¬‡ï¸  
ğŸ¥ Video Portfolio: To be added  
ğŸ“¦ GitHub Profile: <https://github.com/IvanSicaja>  
ğŸ”— LinkedIn: <https://www.linkedin.com/in/ivan-si%C4%8Daja-832682222>  
ğŸ¥ YouTube: <https://www.youtube.com/@ivan_sicaja>

\----------------------------------------------------------------------------------------------------------------

### ğŸ“šğŸ” Project description: â¬‡ï¸â¬‡ï¸â¬‡ï¸

### ğŸ’¡ App Purpose

To provide a **domain-specific conversational AI** capable of answering technical queries in automation, robotics, and programming, with reliable evaluation metrics validating performance.

### ğŸ§  How It Works

This project involved fine-tuning the **Llama 3.2-1B-Instruct** model on a custom dataset of technical questions and answers related to ROS, PLCs, Arduino, and automation. Over four months, the project encompassed full **data preprocessing, tokenization, model training, evaluation, and deployment**. Multiple evaluation metrics, including **BLEU, ROUGE, METEOR, BERTScore, and perplexity**, were applied to ensure the modelâ€™s performance. Additionally, a **custom knowledge chatbot** was implemented to demonstrate practical inference, enabling context-aware responses to user queries.

The project combined **GPU-based fine-tuning in Google Colab A100**, model evaluation pipelines, and deployment-ready scripts for interaction with the fine-tuned model.

**Project Workflow:**

1. **Data Preprocessing:** Cleaning, tokenization, and input formatting.
2. **Model Fine-Tuning:** Training Llama-3.2-1B-Instruct on domain-specific dataset with optimizations for GPU.
3. **Evaluation:** Computing BLEU, ROUGE, METEOR, BERTScore, and perplexity across sample questions.
4. **Deployment:** Creating a chatbot interface for real-time user interaction.
5. **Feedback Loop:** Evaluating answers and adjusting training parameters for improved accuracy.

### âš ï¸ Note

TBD

### ğŸ”§ Tech Stack

**Python, PyTorch, Transformers, Hugging Face Hub, Google Colab, spaCy, NLTK, Pandas, BLEU, ROUGE, METEOR, BERTScore, Llama 3.2-1B, Custom Dataset, Tokenization, Model Fine-Tuning, GPU Acceleration, Evaluation Pipelines, Chatbot Development**

---

### ğŸ“¸ Project Snapshot

TBD.

---

### ğŸ¥ Video Demonstration

TBD.

---


### ğŸ“£ Hashtags Section

**\# #LLM #AI #MachineLearning #DeepLearning #PyTorch #HuggingFace #Transformers #GoogleColab #GPU #FineTuning #Chatbot #NLP #EvaluationMetrics #BLEU #ROUGE #METEOR #BERTScore #DataPreprocessing #Tokenization #ModelTraining #ModelDeployment #DomainSpecificAI #Automation #Robotics #PLC #Arduino**
